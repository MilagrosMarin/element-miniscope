{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DataJoint Elements Tutorial: Miniscope\n",
    "\n",
    "Welcome to the tutorial for DataJoint's open-source data pipeline for miniature\n",
    "fluorescent microscopes (miniscope). This tutorial aims to provide a comprehensive understanding of the\n",
    "open-source data pipeline created using `element-miniscope` for processing\n",
    "and analyzing calcium dynamics in neurons. \n",
    "\n",
    "**In this tutorial, we will cover:**\n",
    "- The basics:\n",
    "  - Differentiating between an Element, module, schema, table, and pipeline.\n",
    "  - How to plot an overview of the pipeline with `dj.Diagram`.\n",
    "- Hands-on interactions with the pipeline:\n",
    "  - Inserting real data into tables.\n",
    "  - Querying table contents.\n",
    "  - Fetching table contents.\n",
    "- A walk-through:\n",
    "  - Processing miniscope imaging data acquired with UCLA miniscope and Miniscope DAQ V4.\n",
    "\n",
    "**Additional Resources:**\n",
    "- [Interactive Tutorials](https://github.com/datajoint/datajoint-tutorials) on `datajoint-python`: Dive deep into DataJoint's fundamentals.\n",
    "- [*`datajoint-python`* Documentation](https://datajoint.com/docs/core/datajoint-python/): Comprehensive documentation on DataJoint for Python.\n",
    "- [Element Miniscope\n",
    "  Documentation](https://datajoint.com/docs/elements/element-miniscope/): Detailed guide\n",
    "  on the DataJoint Element for Miniscope Imaging.\n",
    "\n",
    "Before we jump into the core concepts, let's ensure we have all the necessary packages imported."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datajoint as dj\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine multiple Elements into a pipeline\n",
    "\n",
    "Each DataJoint Element is a modular set of tables that can be combined into a complete\n",
    "pipeline. Here are the definitions for clarity:\n",
    "\n",
    "+ **Module**: In Python, a module is a file containing definitions and statements. In the context of DataJoint, modules often define and structure related database tables.\n",
    "+ **Table**: A structured set of data held within the database. It consists of rows and\n",
    "  columns, much like an Excel spreadsheet.\n",
    "+ **Schema**: Think of a schema as a container or namespace within the database where related tables are grouped together. It helps organize and manage the database structure.\n",
    "+ **Element**: A modular set of related tables. \n",
    "\n",
    "---\n",
    "\n",
    "Each Element contains 1 or more modules, and each module declares its own schema in the database.\n",
    "\n",
    "This tutorial pipeline is assembled from four DataJoint Elements.\n",
    "\n",
    "| Element | Source Code | Documentation | Description |\n",
    "| -- | -- | -- | -- |\n",
    "| Element Lab | [Link](https://github.com/datajoint/element-lab) | [Link](https://datajoint.com/docs/elements/element-lab) | Lab management related information, such as Lab, User, Project, Protocol, Source. |\n",
    "| Element Animal | [Link](https://github.com/datajoint/element-animal) | [Link](https://datajoint.com/docs/elements/element-animal) | General animal metadata and surgery information. |\n",
    "| Element Session | [Link](https://github.com/datajoint/element-session) | [Link](https://datajoint.com/docs/elements/element-session) | General information of experimental sessions. |\n",
    "| Element Miniscope | [Link](https://github.com/datajoint/element-miniscope) | [Link](https://datajoint.com/docs/elements/element-miniscope) |  Miniscope Imaging analysis with CaImAn. |\n",
    "\n",
    "By importing the modules for the first time, the schemas and tables will be created in\n",
    "the database.  Once created, importing modules will not create schemas and tables\n",
    "again, but will allow access to existing schemas/tables.\n",
    "\n",
    "The Elements are imported and activated within the `tutorial_pipeline` script."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tutorial_pipeline import (\n",
    "    lab,\n",
    "    subject,\n",
    "    session,\n",
    "    miniscope,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each Python module (e.g. `subject`) contains a schema object that enables interaction with the schema in the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.schema"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Python classes in the module correspond to a table in the database server."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Diagram\n",
    "\n",
    "Let's plot the diagram of tables within multiple schemas and their dependencies using `dj.Diagram()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(dj.Diagram(subject.Subject) + dj.Diagram(session.Session) + dj.Diagram(miniscope))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding Table Types in DataJoint\n",
    "\n",
    "In the previous cell, we visualized the relationships between various tables in our pipeline using `dj.Diagram`. As you might have noticed, tables have different colors and shapes. This is because, in DataJoint, tables can be of different types, each serving a unique purpose.\n",
    "\n",
    "| Table tier | Color and shape | Description | Practical Example |\n",
    "| -- | -- | -- | -- |\n",
    "| **Manual table** | Green box | Data entered manually, either by hand or with external helper scripts. | A table containing data about individual subjects, like their birth date or sex. |\n",
    "| **Lookup table** | Gray box | Small tables containing general, non-changing information or settings. | A table containing available experimental protocols or animal species. |\n",
    "| **Imported table** | Blue oval | Data automatically ingested but requiring external data. | A table that pulls data from an external file or dataset. |\n",
    "| **Computed table** | Red circle | Data computed entirely within the pipeline. | A table calculating metrics or statistics from previously stored data. |\n",
    "| **Part table** | Plain text | Tables associated with a master table, sharing its tier. | A subtable containing specific measurements for each subject in a master subject table. |\n",
    "\n",
    "### Order matters!\n",
    "\n",
    "The arrangement of tables in the `dj.Diagram` is not arbitrary. It represents the flow of data and dependencies:\n",
    "\n",
    "- **Higher-up Tables**: These are typically your starting point. For instance, before you can insert data about an experimental session, you need to have data about the subject of that session.\n",
    "- **Dependencies**: Tables connected by a line have dependencies. For example, before\n",
    "  you can populate data about an experiment's results, you need to insert data about the\n",
    "  experiment setup.\n",
    "\n",
    "**Quick Check**: Based on the diagram, which tables do you think we would insert data into first?\n",
    "\n",
    "---\n",
    "\n",
    "## Interacting with DataJoint Pipelines\n",
    "\n",
    "DataJoint offers a powerful set of commands that allow us to interact with the pipeline:\n",
    "\n",
    "- **Insert**: Manually add data to a table.\n",
    "- **Populate**: Automatically compute and insert data.\n",
    "- **Query**: Search and filter data.\n",
    "- **Fetch**: Retrieve data for further analysis or visualization.\n",
    "\n",
    "In the upcoming sections, we'll get hands-on with these commands. Let's start by\n",
    "understanding how to manually insert data into our pipeline!\n",
    "\n",
    "---"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert entries into manual tables\n",
    "\n",
    "Manual tables serve as the foundation upon which our pipeline builds. By entering data\n",
    "here, we lay the groundwork for subsequent automated analyses.\n",
    "\n",
    "Let's start with the first table in the schema diagram (i.e. `subject.Subject` table).\n",
    "\n",
    "To know what data to insert into the table, we can view its dependencies and attributes using the `.describe()` and `.heading` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(subject.Subject.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The cells above show all attributes of the subject table.\n",
    "We will insert data into the\n",
    "`subject.Subject` table. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subject.Subject.insert1(\n",
    "    dict(subject=\"subject1\", subject_birth_date=\"2023-01-01\", sex=\"U\")\n",
    ")\n",
    "subject.Subject()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repeat the steps above for the `Session` table and see how the output varies between\n",
    "`.describe` and `.heading`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(session.Session.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `describe`, displays the table's structure and highlights its dependencies, such as its reliance on the `Subject` table. These dependencies represent foreign key references, linking data across tables.\n",
    "\n",
    "On the other hand, `heading` provides an exhaustive list of the table's attributes. This\n",
    "list includes both the attributes declared in this table and any inherited from upstream\n",
    "tables.\n",
    "\n",
    "With this understanding, let's move on to insert a session associated with our subject.\n",
    "\n",
    "We will insert into the `session.Session` table by passing a dictionary to the `insert1` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_key = dict(subject=\"subject1\", session_datetime=\"2023-01-01 00:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.Session.insert1(session_key)\n",
    "session.Session()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introducing the `SessionDirectory` Table\n",
    "\n",
    "Every experimental session produces a set of data files. The `SessionDirectory` table's\n",
    "purpose is to locate these files. It references a directory path relative to a root\n",
    "directory, defined in `dj.config[\"custom\"]`. More\n",
    "information about `dj.config` is provided in the [User\n",
    "Guide](https://datajoint.com/docs/elements/user-guide/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session.SessionDirectory.insert1(dict(**session_key, session_dir=\"session1\"))\n",
    "session.SessionDirectory()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the workflow diagram indicates, the tables in the `probe` schemas need to\n",
    "contain data before the tables in the `ephys` schema accept any data. Let's\n",
    "start by inserting into `probe.Probe`, a table containing metadata about a\n",
    "multielectrode probe. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(miniscope.Recording.describe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniscope.Recording.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The probe metadata is used by the downstream `ProbeInsertion` table which we\n",
    "insert data into in the cells below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.Device.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab.Device.insert1(\n",
    "    dict(\n",
    "        device=\"miniscope A\",\n",
    "        modality=\"miniscope\"\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniscope.Recording.insert1(\n",
    "    dict(\n",
    "        **session_key,\n",
    "        recording_id=1,\n",
    "        device=\"miniscope A\",\n",
    "        acq_software=\"Miniscope-DAQ-V4\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Populate\n",
    "\n",
    "### Automatically populate tables\n",
    "\n",
    "`miniscope.MiniscopeRecordingInfo` is the first table in this pipeline that can be\n",
    "automatically populated using using the `populate()` method.\n",
    "\n",
    "In DataJoint, the `populate()` method is a powerful feature designed to fill tables based on the logic defined in the table's `make` method. Here's a breakdown of its functionality:\n",
    "\n",
    "- **Automation**: Instead of manually inserting data into each table, which can be error-prone and time-consuming, `populate()` automates the insertion based on the dependencies and relationships already established in the schema.\n",
    "\n",
    "- **Dependency Resolution**: Before populating a table, `populate()` ensures all its dependencies are populated. This maintains the integrity and consistency of the data.\n",
    "\n",
    "- **Part Tables**: If a table has part tables associated with it, calling `populate()` on the main table will also populate its part tables. This is especially useful in cases like `ephys.EphysRecording` and its part table `ephys.EphysRecording.EphysFile`, as they are closely linked in terms of data lineage.\n",
    "\n",
    "- **Restriction**: The `populate()` method can be restricted to specific entries. For instance, by providing a `session_key`, we're ensuring the method only operates on the data relevant to that particular session. This is both efficient and avoids unnecessary operations.\n",
    "\n",
    "In the upcoming cells, we'll make use of the `populate()` method to fill the `miniscope.MiniscopeRecordingInfo` table and its part table. Remember, while this operation is automated, it's essential to understand the underlying logic to ensure accurate and consistent data entry.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniscope.RecordingInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniscope.RecordingInfo.File()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniscope.RecordingInfo.populate(session_key, display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's view the information was entered into each of these tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniscope.MiniscopeRecordingInfo()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniscope.MiniscopeRecordingInfo.File()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're almost ready to perform image processing with `CaImAn`. An important step before\n",
    "processing is managing the parameters which will be used in that step. To do so, we will\n",
    "define the `CaImAn` parameters in a dictionary and insert them into a DataJoint table\n",
    "`ProcessingParamSet`. This table keeps track of all combinations of your image\n",
    "processing parameters. You can choose which parameters are used during processing in a\n",
    "later step.\n",
    "\n",
    "Let's view the attributes and insert data into `miniscope.ProcessingParamSet`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "miniscope.ProcessingParamSet.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# insert clustering task manually\n",
    "params_ks = {\n",
    "    \"fs\": 30000,\n",
    "    \"fshigh\": 150,\n",
    "    \"minfr_goodchannels\": 0.1,\n",
    "    \"Th\": [10, 4],\n",
    "    \"lam\": 10,\n",
    "    \"AUCsplit\": 0.9,\n",
    "    \"minFR\": 0.02,\n",
    "    \"momentum\": [20, 400],\n",
    "    \"sigmaMask\": 30,\n",
    "    \"ThPr\": 8,\n",
    "    \"spkTh\": -6,\n",
    "    \"reorder\": 1,\n",
    "    \"nskip\": 25,\n",
    "    \"GPU\": 1,\n",
    "    \"Nfilt\": 1024,\n",
    "    \"nfilt_factor\": 4,\n",
    "    \"ntbuff\": 64,\n",
    "    \"whiteningRange\": 32,\n",
    "    \"nSkipCov\": 25,\n",
    "    \"scaleproc\": 200,\n",
    "    \"nPCs\": 3,\n",
    "    \"useRAM\": 0,\n",
    "}\n",
    "ephys.ClusteringParamSet.insert_new_params(\n",
    "    clustering_method=\"kilosort2\",\n",
    "    paramset_idx=0,\n",
    "    params=params_ks,\n",
    "    paramset_desc=\"Spike sorting using Kilosort2\",\n",
    ")\n",
    "ephys.ClusteringParamSet()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've inserted kilosort parameters into the `ClusteringParamSet` table,\n",
    "we're almost ready to sort our data. DataJoint uses a `ClusteringTask` table to\n",
    "manage which `EphysRecording` and `ClusteringParamSet` should be used during processing. \n",
    "\n",
    "This table is important for defining several important aspects of\n",
    "downstream processing. Let's view the attributes to get a better understanding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ClusteringTask.heading"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ClusteringTask` table contains two important attributes: \n",
    "+ `paramset_idx` \n",
    "+ `task_mode` \n",
    "\n",
    "The `paramset_idx` attribute tracks\n",
    "your kilosort parameter sets. You can choose the parameter set using which \n",
    "you want spike sort ephys data. For example, `paramset_idx=0` may contain\n",
    "default parameters for kilosort processing whereas `paramset_idx=1` contains your custom parameters for sorting. This\n",
    "attribute tells the `Processing` table which set of parameters you are processing in a given `populate()`.\n",
    "\n",
    "The `task_mode` attribute can be set to either `load` or `trigger`. When set to `load`,\n",
    "running the processing step initiates a search for exisiting kilosort output files. When set to `trigger`, the\n",
    "processing step will run kilosort on the raw data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.ClusteringTask.insert1(\n",
    "    dict(\n",
    "        session_key,\n",
    "        insertion_number=1,\n",
    "        paramset_idx=0,\n",
    "        task_mode=\"load\",  # load or trigger\n",
    "        clustering_output_dir=\"processed/subject5/session1/probe_1/kilosort2-5_1\",\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.Clustering.populate(session_key, display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While spike sorting is completed in the above step, you can optionally curate\n",
    "the output of image processing using the `Curation` table. For this demo, we\n",
    "will simply use the results of the spike sorting output from the `Clustering` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.Curation.heading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clustering_key = (ephys.ClusteringTask & session_key).fetch1(\"KEY\")\n",
    "ephys.Curation().create1_from_clustering_task(clustering_key)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the `Curation` table receives an entry, we can populate the remaining\n",
    "tables in the workflow including `CuratedClustering`, `WaveformSet`, and `LFP`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ephys.CuratedClustering.populate(session_key, display_progress=True)\n",
    "ephys.LFP.populate(session_key, display_progress=True)\n",
    "ephys.WaveformSet.populate(session_key, display_progress=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've populated the tables in this workflow, there are one of\n",
    "several next steps. If you have an existing workflow for\n",
    "aligning waveforms to behavior data or other stimuli, you can easily\n",
    "invoke `element-event` or define your custom DataJoint tables to extend the\n",
    "pipeline.\n",
    "\n",
    "In this tutorial, we will do some exploratory analysis by fetching the data from the database and creating a few plots.\n",
    "\n",
    "## Querying Data\n",
    "\n",
    "DataJoint provides a powerful querying system, allowing you to retrieve and work with data stored in your database seamlessly. In this section, we'll explore the fundamental querying concepts.\n",
    "\n",
    "#### What is a Query?\n",
    "\n",
    "- A query is essentially a request for data. With DataJoint, you can craft specific queries to fetch data that meets your criteria from the database.\n",
    "\n",
    "#### The `fetch()` Method\n",
    "\n",
    "- The primary method for retrieving data from a DataJoint table is `fetch()`.\n",
    "- **Default Behavior**: Without any arguments, `fetch()` returns a list of dictionaries. Each dictionary corresponds to an entry in the table.\n",
    "  \n",
    "#### The `fetch1()` Method\n",
    "\n",
    "- For tables with a single entry or when you're only interested in the first entry, use `fetch1()`.\n",
    "- **Default Behavior**: It returns a dictionary of attributes for that one entry.\n",
    "\n",
    "#### Specific Attributes\n",
    "\n",
    "- Both `fetch()` and `fetch1()` can be made more specific by providing attributes.\n",
    "- Example: `fetch1('fps')` will retrieve only the `fps` attribute from the first entry.\n",
    "\n",
    "#### Restricting Queries\n",
    "\n",
    "- Often, you don't want to fetch everything. Instead, you might want data related to a specific subject or session.\n",
    "- DataJoint uses the `&` operator to restrict queries.\n",
    "- Example: To get all session times for `subject5`, you might use:\n",
    "  ```python\n",
    "  subject1_times = (session.Session & \"subject = 'subject1'\").fetch(\"session_datetime\")\n",
    "  ```\n",
    "\n",
    "#### Fetching Primary Keys\n",
    "\n",
    "- Sometimes, you just need the primary keys of entries.\n",
    "- Use the `fetch(\"KEY\")` syntax for this. For instance, `(session.Session).fetch(\"KEY\")`.\n",
    "\n",
    "#### Let's Dive In!\n",
    "\n",
    "Now that we've established the basics, let's delve deeper into querying with some practical examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lfp_average = (ephys.LFP & \"insertion_number = '1'\").fetch1(\"lfp_mean\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the query above, we fetch a single `lfp_mean` attribute from the `LFP` table.\n",
    "We also restrict the query to insertion number 1.\n",
    "\n",
    "Let's go ahead and plot the LFP mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(lfp_average)\n",
    "plt.title(\"Average LFP Waveform for Insertion 1\")\n",
    "plt.xlabel(\"Samples\")\n",
    "plt.ylabel(\"microvolts (uV)\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataJoint queries are a highly flexible tool to manipulate and visualize your data.\n",
    "After all, visualizing traces or generating rasters is likely just the start of\n",
    "your analysis workflow. This can also make the queries seem more complex at\n",
    "first. However, we'll walk through them slowly to simplify their content in this notebook. \n",
    "\n",
    "The examples below perform several operations using DataJoint queries:\n",
    "- Fetch the primary key attributes of all units that are in `insertion_number=1`.\n",
    "- Use **multiple restrictions** to fetch timestamps and create a raster plot.\n",
    "- Use a **join** operation and **multiple restrictions** to fetch a waveform\n",
    "  trace, along with unit data to create a single waveform plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "insert_key = (ephys.ProbeInsertion & \"insertion_number = '1'\").fetch1(\"KEY\")\n",
    "units, unit_spiketimes = (\n",
    "    ephys.CuratedClustering.Unit\n",
    "    & insert_key\n",
    "    & 'unit IN (\"6\",\"7\",\"9\",\"14\",\"15\",\"17\",\"19\")'\n",
    ").fetch(\"unit\", \"spike_times\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.hstack(unit_spiketimes)\n",
    "y = np.hstack([np.full_like(s, u) for u, s in zip(units, unit_spiketimes)])\n",
    "plt.plot(x, y, \"|\")\n",
    "plt.xlabel(\"Time (s)\")\n",
    "plt.ylabel(\"Unit\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_key = (ephys.CuratedClustering.Unit & insert_key & \"unit = '15'\").fetch1(\"KEY\")\n",
    "unit_data = (\n",
    "    ephys.CuratedClustering.Unit * ephys.WaveformSet.PeakWaveform & unit_key\n",
    ").fetch1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sampling_rate = (ephys.EphysRecording & insert_key).fetch1(\n",
    "    \"sampling_rate\"\n",
    ") / 1000  # in kHz\n",
    "plt.plot(\n",
    "    np.r_[: unit_data[\"peak_electrode_waveform\"].size] * 1 / sampling_rate,\n",
    "    unit_data[\"peak_electrode_waveform\"],\n",
    ")\n",
    "plt.xlabel(\"Time (ms)\")\n",
    "plt.ylabel(r\"Voltage ($\\mu$V)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Throughout this notebook, we've used DataJoint to work with database tables and keep\n",
    "data organized and automate analyses to increase efficiency of data processing. We've\n",
    "inserted data into tables, used queries to retrieve, manipulate, and visualize ephys data.\n",
    "\n",
    "Remember, this is just the beginning. As you grow familiar with DataJoint, you'll\n",
    "uncover even more ways to harness its capabilities for your specific research needs. \n",
    "\n",
    "---\n",
    "\n",
    "To run this tutorial notebook on your own data, please use the following steps:\n",
    "- Download the mysql-docker image for DataJoint and run the container according to the\n",
    "  instructions provide in the repository.\n",
    "- Create a fork of this repository to your GitHub account.\n",
    "- Clone the repository and open the files using your IDE.\n",
    "- Add a code cell immediately after the first code cell in the notebook - we will setup\n",
    "  the local connection using this cell. In this cell, type in the following code. \n",
    "\n",
    "```python\n",
    "import datajoint as dj\n",
    "dj.config[\"database.host\"] = \"localhost\"\n",
    "dj.config[\"database.user\"] = \"<your-username>\"\n",
    "dj.config[\"database.password\"] = \"<your-password>\"\n",
    "dj.config[\"custom\"] = {\"imaging_root_data_dir\": \"path/to/your/data/dir\",\n",
    "\"database_prefix\": \"<your-username_>\"}\n",
    "dj.config.save_local()\n",
    "dj.conn()\n",
    "```\n",
    "\n",
    "- Run this code block above and proceed with the rest of the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3p10",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ff52d424e56dd643d8b2ec122f40a2e279e94970100b4e6430cb9025a65ba4cf"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
